{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction"
      ],
      "metadata": {
        "id": "HO3_B5vsx144"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lesson, we will explore how to use Python for text analysis by working with a trial transcript from the *Proceedings of the Old Bailey*. Text analysis is an essential tool for historians, allowing us to uncover patterns and insights from historical documents that would be difficult to identify manually.\n",
        "\n",
        "We will begin by scraping the trial transcript from the *Old Bailey Online*, a valuable digital archive containing the records of over 197,000 criminal trials held at the Central Criminal Court in London between 1674 and 1913. These records provide a rich source of information about crime, justice, and society in early modern London."
      ],
      "metadata": {
        "id": "QMpD95Drx5hz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This hands-on lesson will teach you how to:\n",
        "\n",
        "1.   Scrape text data from a webpage.\n",
        "2.   Clean the text to make it suitable for analysis.\n",
        "3.   Generate and visualize word frequencies to reveal trends in the language used during the trial.\n",
        "\n",
        "By the end of the lesson, you will be able to apply these basic text analysis techniques to other historical documents, helping you to engage with historical sources in new and exciting ways.\n",
        "\n",
        "This lesson was inspired by [this sequence of lessons](https://programminghistorian.org/en/lessons/from-html-to-list-of-words-1) on the [*Programming Historian*](https://programminghistorian.org/)!"
      ],
      "metadata": {
        "id": "47AZV0v7yDpK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About the *Proceedings of the Old Bailey*"
      ],
      "metadata": {
        "id": "EG2YdJk7yOV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *Proceedings of the Old Bailey* is a major online resource for historians, providing access to detailed accounts of trials held at London's central criminal court from 1674 to 1913. It includes witness testimony, defense statements, verdicts, and sentencing information. The website also offers powerful search tools, allowing users to investigate topics ranging from crime and punishment to class, gender, and race in early modern London.\n",
        "\n",
        "The transcript we will analyze today is from the trial of Benjamin Bowsey, a participant in the 1780 [Gordon Riots](https://en.wikipedia.org/wiki/Gordon_Riots), accused of rioting and the destruction of property. This document, like many others in the *Old Bailey Online*, offers valuable insights into the social and political tensions of the time."
      ],
      "metadata": {
        "id": "Q6ySCRNgyTQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Scraping text from the trial transcript"
      ],
      "metadata": {
        "id": "R7G37ij9yzeD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "asyabf4fxW9-"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Open the HTML file and parse it\n",
        "# REMEMBER TO CHANGE THIS PATH SO THAT IT POINTS AT A FILE YOU HAVE ACCESS TO!\n",
        "with open('/content/drive/MyDrive/Vanderbilt/Teaching/HIST 1354: Digital Methods for History Research/riot_trial.html', 'r', encoding='utf-8') as file:\n",
        "    soup = BeautifulSoup(file, 'html.parser')\n",
        "\n",
        "# Extract all the paragraph tags which contain the main text of the trial\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Concatenate the text content of all paragraphs into one string\n",
        "raw_text = ' '.join([para.get_text() for para in paragraphs])\n",
        "\n",
        "# Print the first 500 characters to see what the text looks like\n",
        "print(raw_text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   `BeautifulSoup` is used to parse the HTML content.\n",
        "\n",
        "*   We locate all the `<p>` tags since visual inspection of the HTML indicated that the trial transcript is contained within those tags.\n",
        "\n",
        "*   The content of each paragraph is extracted, combined into a single string (`raw_text`), and we print the first 500 characters to verify the extraction."
      ],
      "metadata": {
        "id": "TBeUJhy4zQee"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Cleaning the text"
      ],
      "metadata": {
        "id": "RRFZVSGaz4I0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to clean the scraped text. We will remove unwanted characters, such as special symbols, punctuation, and extra spaces, and make all text lowercase for uniformity."
      ],
      "metadata": {
        "id": "5-WHDMDRz6ZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download the stopwords from NLTK if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Define a more advanced cleaning function with stop word removal\n",
        "def clean_text(text):\n",
        "    # Remove any non-alphabetic characters (numbers, punctuation, etc.)\n",
        "    cleaned = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    cleaned = cleaned.lower()\n",
        "    # Remove extra whitespace\n",
        "    cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "\n",
        "    # Split the text into individual words\n",
        "    words = cleaned.split()\n",
        "\n",
        "    # Remove stop words using NLTK's stop word list for English\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Return the cleaned and filtered text as a single string\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Clean the extracted raw text with stop word removal\n",
        "cleaned_text = clean_text(raw_text)\n",
        "\n",
        "# Print the first 500 characters of the cleaned text to check\n",
        "print(cleaned_text[:500])"
      ],
      "metadata": {
        "id": "3XhQ-rW_z8nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   The re.sub() function is used to remove any character that is not a letter or whitespace.\n",
        "\n",
        "*   The clean_text function also converts the text to lowercase and removes excess spaces.\n",
        "\n",
        "*   After cleaning the text by removing non-alphabetic characters and converting it to lowercase, we split the text into individual words.\n",
        "\n",
        "*   We use the `nltk.corpus.stopwords` list to remove common English stop words.\n",
        "\n",
        "*   The cleaned text is returned as a single string of meaningful words, ready for further analysis."
      ],
      "metadata": {
        "id": "dJv1RmHG0DK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Word frequency analysis"
      ],
      "metadata": {
        "id": "V5wYpeWA0W1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have cleaned the text, we can generate a list of word frequencies. This will help us understand which words are most common in the trial transcript."
      ],
      "metadata": {
        "id": "qdaiHzCV0wDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Split the cleaned text into individual words\n",
        "words = cleaned_text.split()\n",
        "\n",
        "# Use Counter to count the occurrences of each word\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# Display the 10 most common words\n",
        "word_freq.most_common(10)"
      ],
      "metadata": {
        "id": "hehCnoEL0yM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   We split the cleaned text into words using `.split()`, which creates a list of words.\n",
        "\n",
        "*   The `Counter` class from the `collections` module counts the frequency of each word.\n",
        "\n",
        "*   We print the 10 most common words using `most_common(10)`."
      ],
      "metadata": {
        "id": "xfnsQAQp1eE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualizing word frequencies"
      ],
      "metadata": {
        "id": "qr_VaHD01tRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you want to visualize the word frequencies, you can use the `matplotlib` library to create a bar chart."
      ],
      "metadata": {
        "id": "Aa2Nof1K1wkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the 10 most common words and their frequencies\n",
        "common_words = word_freq.most_common(10)\n",
        "words, counts = zip(*common_words)\n",
        "\n",
        "# Create a bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(words, counts)\n",
        "plt.title('Top 10 Most Common Words in Trial Transcript')\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SNLYBgMg1z4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   This code takes the top 10 most frequent words and their counts.\n",
        "\n",
        "*   It uses `matplotlib` to generate a simple bar chart visualizing the word frequencies.\n",
        "\n",
        "*   The `xticks(rotation=45)` rotates the word labels to make them easier to read."
      ],
      "metadata": {
        "id": "OeAdFAXU13ip"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "qzPaaMDD2QME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lesson, you learned how to scrape, clean, and analyze a historical trial transcript using Python. The techniques covered here provide a foundation for text analysis in digital history, enabling you to extract meaningful insights from primary source documents. As you continue working with historical texts, consider how these computational methods can be applied to different types of documents and how they might complement more traditional historical approaches.\n",
        "\n",
        "The combination of digital tools and historical research opens new possibilities for exploring the past, allowing us to engage with sources in ways that were previously unimaginable."
      ],
      "metadata": {
        "id": "Q-9iVSmd2Ra-"
      }
    }
  ]
}